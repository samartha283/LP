

# Importing Necesarry Packages
import numpy as np    #NumPy is a Python library used for working with arrays.
from numpy.ma.core import argmax
import pandas as pd import pandas as pd   # Pandas is a Python library used for working with data sets.
from matplotlib import cm    #Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns  #Seaborn is a library for making statistical graphics in Python
#import os
import time
from sklearn.metrics import confusion_matrix, accuracy_score, auc  #  sklearn : tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.callbacks import EarlyStopping
from keras import models
from keras import layers
from keras.datasets import imdb


# Loading the dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data()
X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)

# Exploring the Data
print("Training data: ")
print(X.shape)
print(y.shape)
print("Classes: ")
print(np.unique(y))
print("Number of words: ")
print(len(np.unique(np.hstack(X))))
print("Review length: ")
result = [len(x) for x in X]
print("Mean %.2f words (%f)" % (np.mean(result), np.std(result))) # Ploting the review length
plt.boxplot(result)
plt.show()



#Overall, this function is a common preprocessing step for text data in natural language processing tasks, converting text data into a format suitable for input into neural networks or other machine learning algorithm
def vectorize_sequences(sequences, dimension=5000): # Function for vectorising data  (takes a list of sequences (presumably representing text data) and converts them into a binary matrix representation suitable for input into a neural network. Let's break down the function:)
    results = np.zeros((len(sequences), dimension)) # Creating an all-zero matrix of shape (len(sequences), dimension)
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.  # Set specific indices of results[i] to 1s
    return results


# Creating Training and Testing Sets and Preprocessing them
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=5000)
# Our vectorized training data
x_train = vectorize_sequences(train_data)
# Our vectorized test data
x_test = vectorize_sequences(test_data)
# Our vectorized labels one-hot encoder
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')





# Creating the DNN Model
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(5000,)))   #relu : The function returns 0 if it receives any negative input, but for any positive value x it returns that value back
model.add(layers.Dense(32, activation='relu',))              ##dense:the layer that contains all the neurons that are deeply connected within themselves.
model.add(layers.Dense(1, activation='sigmoid'))


#Set validation set aside
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]



# Compiling Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
start_time_m1 = time.time()

#This line trains the model using the fit method. The partial_x_train and partial_y_train datasets are used for training, with 20 epochs (i.e., the entire dataset is passed through the model 20 times during training), a batch size of 512 (i.e., the model is updated after every 512 samples), and the validation data (x_val, y_val) is used to evaluate the model's performance during training.
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

total_time_m1 = time.time() - start_time_m1
print("The Dense Convolutional Neural Network 1 layer took %.4f seconds to train." % (total_time_m1))  #
This code snippet compiles and trains the previously defined model (model) using the training data (partial_x_train and partial_y_train) and evaluates its performance on the validation data (x_val and y_val). Let's break it down:

python
Copy code
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
This line compiles the model. The optimizer='adam' argument specifies the Adam optimizer, which is a popular optimization algorithm for training neural networks. The loss='binary_crossentropy' argument specifies the loss function used during training. Since this is a binary classification task, binary cross-entropy is a common choice for the loss function. Additionally, the metrics=['acc'] argument specifies that we want to track the accuracy metric during training.

python
Copy code
start_time_m1 = time.time()
This line records the current time before starting the training process to measure the training time.

python
Copy code
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
This line trains the model using the fit method. The partial_x_train and partial_y_train datasets are used for training, with 20 epochs (i.e., the entire dataset is passed through the model 20 times during training), a batch size of 512 (i.e., the model is updated after every 512 samples), and the validation data (x_val, y_val) is used to evaluate the model's performance during training.

python
Copy code
total_time_m1 = time.time() - start_time_m1
This line calculates the total training time by subtracting the start time from the current time.

python
Copy code
print("The Dense Convolutional Neural Network 1 layer took %.4f seconds to train." % (total_time_m1))
This line prints the total training time of the model.

history_dict = history.history
history_dict.keys()


acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

# Plotting model loss
plt.plot(epochs, loss, 'bo', label='Training loss') # "bo" is for "blue dot"   A decreasing training loss suggests that the model is learning to fit the training data better over time. 
plt.plot(epochs, val_loss, 'b', label='Validation loss') # b is for "solid blue line"   This indicates that the model is becoming less effective at generalizing to unseen data, which is a common symptom of overfitting.
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


plt.clf()   # clear figure
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']

# Plotting model accuracy
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


# Model Summary
print(model.summary())

# Predictions
pred = model.predict(x_test)
classes_x=np.argmax(pred,axis=1)
accuracy_score(y_test,classes_x)

#Confusion Matrix
conf_mat = confusion_matrix(y_test, classes_x)
print(conf_mat)

conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
sns.heatmap(conf_mat_normalized)
plt.ylabel('True label')


#Dense with Two Layer
model2 = models.Sequential()
model2.add(layers.Dense(32, activation='relu', input_shape=(5000,)))
model2.add(layers.Dense(32, activation='relu'))
model2.add(layers.Dense(32, activation='relu'))
model2.add(layers.Dense(1, activation='sigmoid'))


# Compiling Model
model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
start_time_m2 = time.time()
history= model2.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
total_time_m2 = time.time() - start_time_m2
print("The Dense Convolutional Neural Network 2 layers took %.4f seconds to train." % (total_time_m2))
